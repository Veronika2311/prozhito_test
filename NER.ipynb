{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "NER.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Veronika2311/prozhito_test/blob/main/NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_welFa6AO0N"
      },
      "source": [
        "# Прожито"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwl-ehjHAO0N"
      },
      "source": [
        "Возьмите дамп «прожито»:\n",
        "\n",
        "https://www.dropbox.com/sh/8vfjjt8107sv9r3/AADOVR795M..\n",
        "\n",
        "Выберите из него небольшой когерентный кусочек при помощи тулзы для работы с дампом (https://github.com/kilomeow/prozhito-tools)\n",
        "\n",
        "Особенно интересно выбрать кусок максимально удаленный от сегодня или с по какой-то причине непохожим на сегодняшний интернет языком\n",
        "\n",
        "Расскажите коротко, почему вы выбрали такой кусок\n",
        "\n",
        "Разметьте его каким-то готовым NER решением\n",
        "\n",
        "Предложите подход к оценке и анализу полученной разметки. Чего вы ждёте от неё, какие машина допустила ошибки, как их может показаться можным исправить. Почему современный NER допускает такие ошибки? Что вы вообще думаете об этом и смежном?\n",
        "\n",
        "Если вдруг успеете, сделайте часть такого анализа."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq6d_EH2AO0P"
      },
      "source": [
        "## Отрывок"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE7m1qJPAO0P"
      },
      "source": [
        "Теггеры обучаются на разных данных, но в основном \"из коробки\" их целью не является разметка именно дневниковых записей (или же кхудожественной литературы). Наташа, к примеру, обучена на корпусе новостей. Соответственно, при разметке дневников может появиться сразу несколько проблем. \n",
        "1. Язык и синтаксис другой эпохи (в Прожито, и даже в нашем отрывке есть дневники с 17го века).\n",
        "2. Нетипичные сокращения и обозначения людей и мест буквально в ннесколько букв (а также разные обозначения одного и того же человека)\n",
        "3. Вышедшие из употребления названия, имена и аббревиатуры\n",
        "\n",
        "Поэтому подобрать ровно одну запись так, чтобы в ней проявлялись все эти отличительные черты, довольно сложно. Но можно, вероятно, взять несколько небольших отрывков с интересующими нас чертами и посмротреть на то, как они будут размечены."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RZvs3n7AO0R"
      },
      "source": [
        "TEXT1 = '''Позавчера я сдал все дела и снова стал работать рядовым контролером. Заходил в ЭРО. Там мне выписали командировку в Москву, Житомир и Борисполь. \n",
        "Но когда я стал разговаривать с нач-ком ОТК, он своего согласия не дал. Я его все два дня уговорить пытался. А причина этому опять та, что Кравец остался за парторга \n",
        "цеха, а я буду снова за старшего, но в другой смене. Не хочется, но ничего не сделаешь.    \n",
        "Черт с ним! Лишь бы мне только отпуск в сентябре вырвать, да на пароходе скататься. И дом суметь перед отпуском отремонтировать.'''"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdIzmTp8egOP"
      },
      "source": [
        "TEXT2 = '''Я купила себе шёлковый шарфик для шубы и с нетерпением ждала вечера, хотя, в то же время, боялась, что Конст. Хр. будет со мной скучать. О чём я буду с ним говорить? \n",
        "Приехав домой из техникума, я стала торопливо собираться, т. к. времени осталось мало; погладила платье, сделала маникюр, причесалась и, вдруг, звонок. \n",
        "Шура пошёл открывать – это был Аджемов, он пришёл, чтобы предупредить меня, что вместо 20-го, ему удалось достать билеты на 28-е, а на сегодняшний вечер он взял билеты на «Бедную невесту» Островского. \n",
        "Аджемов был очень мил и внимателен. \n",
        "Мы доехали до Никитской пл., затем пересели на другой трамвай, причём К. Х. хотел, т. к. времени было много, обязательно пройтись пешком. \n",
        "Мы встали у Зоопарка и, пройдя мимо пруда, на котором каталась Кити Толстого, пошли к Краснопресненской заставе.'''"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPU7ELCEE785"
      },
      "source": [
        "TEXT3 = '''Божиею милостью Великого Государя, Царя и Великого Князя, Василия Ивановича, всей Руси Самодержца, Владимирского, \n",
        "Московского, Новгородского, Царя Казанского, Царя Астраханского, Царя Сибирского, Государя Псковского и Великого Князя \n",
        "Смоленского, Болгарского и иных, Государя и Великого Князя Новагорода Низовской земли, Черниговского, Рязанского, \n",
        "Ростовского, Ярославского, Белозерского, Удорского, Обдорского, Кондийского и всей Северной страны Повелителя, \n",
        "Государя Северской земли, Карталинских и Грузинских Царей и Кабардинской земли, Черкасских и Горских Князей и иных \n",
        "многих государств Государя и Обладателя, Его Царского Величества от старшины и воеводы и наместника Смоленского, \n",
        "Михаила Борисовича Шеина, в Оршу, Оршанскому старосте Андрею Ивановичу Сапеге. Августа 22 дня писал ты ко мне в листе \n",
        "своем с посланными своими, с Павликом Полежоным, что Государь ваш, Король Сигизмунд, приехал в Оршу и чтобы мне уведомить \n",
        "тебя о своевольных людях Государя вашего, учинивших обиды везде на границе Смоленской. И я неоднократно прежде сего писал \n",
        "к тебе, когда у Великого Государя нашего Царя и Великого Князя, Василия Ивановича, всей Русии Самодержца, в Москве, \n",
        "были послы вашего Короля, Николай Олесницкий и Александр Гонсевский и гонцы Станислав Витовский и Князь Ян Соколинский, \n",
        "и постановили между Великим Государем вашим, Королем Сигизмундом, и между их великими Государствами союз и мир, утвердив \n",
        "его союзными грамотами и крестным целованием на том*,* чтобы в перемирное время разрыва и войны никому не начинать, а \n",
        "которые люди Государя вашего в государстве нашего Государя, с панами и ротмистрами и со множеством Польских и Литовских \n",
        "людей, опустошают вместе с Самозванцем земли Государя нашего и проливают Христианскую кровь, и всех бы тех людей, по \n",
        "общему согласию, из государств Великого Государя нашего вашему Государю, Королю Сигизмунду, вывести, и впредь, в перемирные \n",
        "годы, не пропускать никого военным обычаем из Государств вашего Государя, и обманщиков, которые изменнически называют себя \n",
        "потомками Великих Государей, не держать и никакого вспомоществования им не оказывать. И после того посольского постановления\n",
        "и утверждения, Польские и Литовские люди и доныне проливают Христианскую кровь в Государств нашего Государя и опустошают \n",
        "земли Государя нашего. А посланных твоих я отправил к тебе, не задерживая. Писано в Великого \n",
        "Государя нашего, Его Царского Величества, отчине, в городе Смоленске, 1609 года, Августа 22 дня.'''"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnUePPSjAO0R"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxivJqllAO0S"
      },
      "source": [
        "### Natasha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDNG_MNM1HAV"
      },
      "source": [
        "Наташа -- наверное, первое, что могло бы прийти в голову при мысли о распознавании именных сущностей для русского языка. Легче и быстрее, чем DeepPavlov, но при этом не слишком сильно должна уступать ему в точности как минимум на новостных корпусах. Стандартная трёхчастная разметка человек-место-организация, что, с одной сстороны, неплохо для стандартного определения точности, с другой -- часто у именованных сущностей более сложная структура, например, должность + имя или упоминание организации и нескольких её подразделений, которые тоже являются организациями (например, ФиКЛ ФГН НИУ ВШЭ). С третьей -- дата и время, если нам нужны и они тоже, никак не могут быть распознаны с помощью этой библиотеки.\n",
        "\n",
        "Поэтому можно предположить, что в некоторых случаях разметка может разбить одно длинное наименование на несколько или недоразметить слово из этого наименования, как это было в примерах в документации."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJLpRvKTBHpH",
        "outputId": "74c1c1ff-c5a6-46f9-a766-3bf7caac94b3"
      },
      "source": [
        "!pip install natasha"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting natasha\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/8e/ab0745100be276750fb6b8858c6180a1756696572295a74eb5aea77f3bbd/natasha-1.4.0-py3-none-any.whl (34.4MB)\n",
            "\u001b[K     |████████████████████████████████| 34.4MB 110kB/s \n",
            "\u001b[?25hCollecting navec>=0.9.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/c1/771ec5565f0ce24874d7fd325b429f9caa80517a40d2e4ce5705120591f3/navec-0.10.0-py3-none-any.whl\n",
            "Collecting slovnet>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/3b/f1ef495be8990004959dd0510c95f688d1b07529f6a862bc56a405770b26/slovnet-0.5.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.8MB/s \n",
            "\u001b[?25hCollecting pymorphy2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/57/b2ff2fae3376d4f3c697b9886b64a54b476e1a332c67eee9f88e7f1ae8c9/pymorphy2-0.9.1-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.2MB/s \n",
            "\u001b[?25hCollecting ipymarkup>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bf/9b/bf54c98d50735a4a7c84c71e92c5361730c878ebfe903d2c2d196ef66055/ipymarkup-0.9.0-py3-none-any.whl\n",
            "Collecting yargy>=0.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/46/bc1a17200a55f4b0608f39ac64f1840fd4a52f9eeea462d9afecbf71246b/yargy-0.15.0-py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.0MB/s \n",
            "\u001b[?25hCollecting razdel>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/2c/664223a3924aa6e70479f7d37220b3a658765b9cfe760b4af7ffdc50d38f/razdel-0.5.0-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from navec>=0.9.0->natasha) (1.18.5)\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/79/bea0021eeb7eeefde22ef9e96badf174068a2dd20264b9a378f2be1cdd9e/pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2MB 42.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2->natasha) (0.6.2)\n",
            "Collecting intervaltree>=3\n",
            "  Downloading https://files.pythonhosted.org/packages/50/fb/396d568039d21344639db96d940d40eb62befe704ef849b27949ded5c3bb/intervaltree-3.1.0.tar.gz\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.2.2)\n",
            "Building wheels for collected packages: intervaltree\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26100 sha256=b0fd33642df66a0130834f5fef319f3386ef12622084368660b5f52c5ce9f978\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/f2/66/e9c30d3e9499e65ea2fa0d07c002e64de63bd0adaa49c445bf\n",
            "Successfully built intervaltree\n",
            "Installing collected packages: navec, razdel, slovnet, dawg-python, pymorphy2-dicts-ru, pymorphy2, intervaltree, ipymarkup, yargy, natasha\n",
            "  Found existing installation: intervaltree 2.1.0\n",
            "    Uninstalling intervaltree-2.1.0:\n",
            "      Successfully uninstalled intervaltree-2.1.0\n",
            "Successfully installed dawg-python-0.7.2 intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.4.0 navec-0.10.0 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 razdel-0.5.0 slovnet-0.5.0 yargy-0.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDmWTSkmAO0S"
      },
      "source": [
        "from natasha import (\n",
        "    Segmenter,\n",
        "    MorphVocab,\n",
        "    \n",
        "    NewsEmbedding,\n",
        "    NewsMorphTagger,\n",
        "    NewsSyntaxParser,\n",
        "    NewsNERTagger,\n",
        "    \n",
        "    PER,\n",
        "    NamesExtractor,\n",
        "\n",
        "    Doc\n",
        ")\n",
        "segmenter = Segmenter()\n",
        "morph_vocab = MorphVocab()\n",
        "\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "syntax_parser = NewsSyntaxParser(emb)\n",
        "ner_tagger = NewsNERTagger(emb)\n",
        "\n",
        "names_extractor = NamesExtractor(morph_vocab)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kY9Gm9TXAO0S"
      },
      "source": [
        "def natasha_result(TEXT):\n",
        "      doc = Doc(TEXT)\n",
        "      doc.segment(segmenter)\n",
        "      doc.tag_ner(ner_tagger)\n",
        "      display(doc.spans[:5])\n",
        "      doc.ner.print()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yc4xHmtBW2f"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJSUTi4xAO0S"
      },
      "source": [
        "### PullEnti"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz-i-Ju4zFOj"
      },
      "source": [
        "С одной стороны, много тегов. И нужных, и не очень нужных именно для дневниковых записей. Однако если просмотреть их бегло, то понятно, что записи в дневниках попадаются самой разной структуры, и где-то могли бы встретится, к примеру, телефоны. Однако в большей степени многие теги, возможно, если бы не были лишними здесь, то по крайней мере не являлись бы наиболее необходимыми. При этом плюсом данного разметчика является то, что кроме отдельных сущностей, он определяет и отношения между ними, так, что это не просто географический объект, но у него есть тип, и к этому же объекту првязываются разные именования одного и того же объекта, что может быть преимуществом и сделать разметку более удобной. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Putgvd0EIXv",
        "outputId": "cfd8d84c-7e56-4d1d-db62-488ea3198ff3"
      },
      "source": [
        "!pip install pullenti-wrapper\n",
        "!pip install graphviz"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pullenti-wrapper\n",
            "  Downloading https://files.pythonhosted.org/packages/68/78/1534c27b2e9cb27d81c3730a0ea8be1e9c64510a9d9d8ffb9898f7d6d8c1/pullenti_wrapper-0.9.0-py3-none-any.whl\n",
            "Collecting pullenti==3.23\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/23/68c539ab43533632ec8a9bacb7ae1f98228e37a140541b97084b406a0ef6/pullenti-3.23-py3-none-any.whl (15.3MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3MB 318kB/s \n",
            "\u001b[?25hCollecting pullenti-client==0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5e/ab/0c026b5a7adbf9172737b14285c222351fd567b5bc5b9f120b2e57b0c826/pullenti_client-0.6.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pullenti-client==0.6.0->pullenti-wrapper) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pullenti-client==0.6.0->pullenti-wrapper) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pullenti-client==0.6.0->pullenti-wrapper) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pullenti-client==0.6.0->pullenti-wrapper) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pullenti-client==0.6.0->pullenti-wrapper) (2020.6.20)\n",
            "Installing collected packages: pullenti, pullenti-client, pullenti-wrapper\n",
            "Successfully installed pullenti-3.23 pullenti-client-0.6.0 pullenti-wrapper-0.9.0\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (0.10.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prBqLB7ZAO0T"
      },
      "source": [
        "from pullenti_wrapper.processor import (\n",
        "    Processor,\n",
        "    MONEY,\n",
        "    URI,\n",
        "    PHONE,\n",
        "    DATE,\n",
        "    KEYWORD,\n",
        "    DEFINITION,\n",
        "    DENOMINATION,\n",
        "    MEASURE,\n",
        "    BANK,\n",
        "    GEO,\n",
        "    ADDRESS,\n",
        "    ORGANIZATION,\n",
        "    PERSON,\n",
        "    MAIL,\n",
        "    TRANSPORT,\n",
        "    DECREE,\n",
        "    INSTRUMENT,\n",
        "    TITLEPAGE,\n",
        "    BOOKLINK,\n",
        "    BUSINESS,\n",
        "    NAMEDENTITY,\n",
        "    WEAPON,\n",
        ")\n",
        "\n",
        "processor = Processor([PERSON, ORGANIZATION, GEO, DATE, MONEY])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHaoMjdyhItu"
      },
      "source": [
        "def pullenty_result(TEXT):\n",
        "    text = TEXT\n",
        "    result = processor(text)\n",
        "    result.graph"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5H86N5mAO0U"
      },
      "source": [
        "### Stanza"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmRn37TZwqLa"
      },
      "source": [
        "For packages with 4 named entity types, supported types include PER (Person), LOC (Location), ORG (Organization) and MISC (Miscellaneous).\n",
        "\n",
        "Всего четыре тега, три из них стандартны, и этого мало, если мы хотим кроме людей, мест и организаций, выделять ещё дату и время, к примеру. Однако из плюсов здесь есть выделение всего прочего в отдельную категорию. \n",
        "\n",
        "Между прочим, у stanza даже есть специальная модель под \"old russian\", но, к сожалению, именно распознавания именованных сущностей для этой модели нет. Зато есть лемматизация и POS-теги."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYAj0_G6EjdL",
        "outputId": "b19eedbb-7a69-477b-e22a-710ba104db09"
      },
      "source": [
        "!pip install stanza"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stanza\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/8b/3a9e7a8d8cb14ad6afffc3983b7a7322a3a24d94ebc978a70746fcffc085/stanza-1.1.1-py3-none-any.whl (227kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanza) (1.18.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanza) (3.12.4)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from stanza) (1.7.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanza) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (50.3.2)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (1.24.3)\n",
            "Installing collected packages: stanza\n",
            "Successfully installed stanza-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcSM1-RMAO0U",
        "outputId": "b14fe098-315d-4112-f6cf-dd244c0f85a2"
      },
      "source": [
        "import stanza\n",
        "stanza.download('ru')\n",
        "def stanza_result(text):\n",
        "  nlp = stanza.Pipeline(lang='ru', processors='tokenize,ner')\n",
        "  doc = nlp(text)\n",
        "  print(*[f'entity: {ent.text}\\ttype: {ent.type}' for sent in doc.sentences for ent in sent.ents], sep='\\n')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 22.6MB/s]                    \n",
            "2020-11-20 03:37:28 INFO: Downloading default packages for language: ru (Russian)...\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/ru/default.zip: 100%|██████████| 591M/591M [05:37<00:00, 1.75MB/s]\n",
            "2020-11-20 03:43:15 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zom1bcLdAO0U"
      },
      "source": [
        "### Polyglot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUF-rm-iixmk"
      },
      "source": [
        "Полиглот также размечает не так много категорий, если посмотреть на примеры, делает это не всегда логично. Лицензия не позволяет использование в коммерческих проектах. В целом кажется не идеальным вариантом для решения именно этой ззадачи."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UW9EH47wHKME",
        "outputId": "ece23aef-6996-4c9a-9a70-b48c02fd630a"
      },
      "source": [
        "!pip3 install -U git+https://github.com/aboSamoor/polyglot.git@master\n",
        "!polyglot download embeddings2.ru ner2.ru\n",
        "from polyglot.text import Text"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/aboSamoor/polyglot.git@master\n",
            "  Cloning https://github.com/aboSamoor/polyglot.git (to revision master) to /tmp/pip-req-build-os2iuphm\n",
            "  Running command git clone -q https://github.com/aboSamoor/polyglot.git /tmp/pip-req-build-os2iuphm\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from polyglot==16.7.4) (0.35.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.7.3 in /usr/local/lib/python3.6/dist-packages (from polyglot==16.7.4) (1.15.0)\n",
            "Collecting futures>=2.1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/05/80/f41cca0ea1ff69bce7e7a7d76182b47bb4e1a494380a532af3e8ee70b9ec/futures-3.1.1-py3-none-any.whl\n",
            "Collecting morfessor>=2.0.2a1\n",
            "  Downloading https://files.pythonhosted.org/packages/39/e6/7afea30be2ee4d29ce9de0fa53acbb033163615f849515c0b1956ad074ee/Morfessor-2.0.6-py3-none-any.whl\n",
            "Collecting PyICU>=1.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/46/fa08c8efae2951e67681ec24319f789fc1a74e2096dd74373e34c79319de/PyICU-2.6.tar.gz (233kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 6.8MB/s \n",
            "\u001b[?25hCollecting pycld2>=0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/d2/8b0def84a53c88d0eb27c67b05269fbd16ad68df8c78849e7b5d65e6aec3/pycld2-0.41.tar.gz (41.4MB)\n",
            "\u001b[K     |████████████████████████████████| 41.4MB 105kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from polyglot==16.7.4) (1.18.5)\n",
            "Building wheels for collected packages: polyglot, PyICU, pycld2\n",
            "  Building wheel for polyglot (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for polyglot: filename=polyglot-16.7.4-py2.py3-none-any.whl size=70643 sha256=b3e5250265ceb8b4309c76aa83192a7a7ce048f472dd177610709b2dc3408196\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h2w8lqp_/wheels/42/d9/73/345c7ae8554299ff8b31635d64eb8455fd591385fa734cdbef\n",
            "  Building wheel for PyICU (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyICU: filename=PyICU-2.6-cp36-cp36m-linux_x86_64.whl size=1288359 sha256=dae074df980b071f7874a1a384c533863e3cf119457ba6268c2a0b0bfe168722\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/21/2f/1c91831e8a93537ab21f6b4b935781b681104635fdb0315791\n",
            "  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycld2: filename=pycld2-0.41-cp36-cp36m-linux_x86_64.whl size=9833531 sha256=a3935951c779936f66960a4612205be373c8a8fd52229c30789fd0e68e4beea3\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/8f/e9/08a1a8932a490175bd140206cd86a3dbcfc70498100de11079\n",
            "Successfully built polyglot PyICU pycld2\n",
            "Installing collected packages: futures, morfessor, PyICU, pycld2, polyglot\n",
            "Successfully installed PyICU-2.6 futures-3.1.1 morfessor-2.0.6 polyglot-16.7.4 pycld2-0.41\n",
            "[polyglot_data] Downloading package embeddings2.ru to\n",
            "[polyglot_data]     /root/polyglot_data...\n",
            "[polyglot_data] Downloading package ner2.ru to /root/polyglot_data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "jhXo_ZPTAO0V"
      },
      "source": [
        "def polyglot_result(TEXT):\n",
        "    for ent in Text(TEXT).entities:\n",
        "        print(ent[0],ent.tag)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K18HY-ZaAO0V"
      },
      "source": [
        "### DeepPavlov"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvcX6NjElWn3"
      },
      "source": [
        "Одно из известных \"тяжёлых\" решений на трансформерах, которое представляется создателями как более академическое и исследовательское, чем как коммерческое. Из плюсов также открытый исходный код. Из минусов -- очень тяжёлое всё, занимает много и памяти, и вычислительных ресурсов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3VIs-MnkHfl9",
        "outputId": "a2c448cd-44e9-4a6b-f374-d4b008535fa3"
      },
      "source": [
        "!pip3 install deeppavlov\n",
        "!python3 -m deeppavlov install ner_ontonotes\n",
        "!python -m deeppavlov install ner_ontonotes_bert"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deeppavlov\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/28/cd11453408f56d8ed003c2636a452b362dda52110593abab98465a2d96fc/deeppavlov-0.13.0-py3-none-any.whl (968kB)\n",
            "\u001b[K     |████████████████████████████████| 972kB 4.2MB/s \n",
            "\u001b[?25hCollecting pyopenssl==19.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/de/f8342b68fa9e981d348039954657bdf681b2ab93de27443be51865ffa310/pyOpenSSL-19.1.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pymorphy2-dicts-ru in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (2.4.417127.4579844)\n",
            "Collecting pandas==0.25.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/3f/f6a428599e0d4497e1595030965b5ba455fd8ade6e977e3c819973c4b41d/pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4MB 20.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (3.0.12)\n",
            "Collecting pydantic==1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/24/e78cf017628e7eaed20cb040999b1ecc69f872da53dfd0d9aed40c0fa5f1/pydantic-1.3-cp36-cp36m-manylinux2010_x86_64.whl (7.3MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3MB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: click==7.1.2 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (7.1.2)\n",
            "Collecting numpy==1.18.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/e6/45f71bd24f4e37629e9db5fb75caab919507deae6a5a257f9e4685a5f931/numpy-1.18.0-cp36-cp36m-manylinux1_x86_64.whl (20.1MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1MB 1.4MB/s \n",
            "\u001b[?25hCollecting aio-pika==6.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/07/196a4115cbef31fa0c3dabdea146f02dffe5e49998341d20dbe2278953bc/aio_pika-6.4.1-py3-none-any.whl (40kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm==4.41.1 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (4.41.1)\n",
            "Collecting requests==2.22.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.0MB/s \n",
            "\u001b[?25hCollecting ruamel.yaml==0.15.100\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/9f/83bb34eaf84032b0b54fcc4a6aff1858572d279d65a301c7ae875f523df5/ruamel.yaml-0.15.100-cp36-cp36m-manylinux1_x86_64.whl (656kB)\n",
            "\u001b[K     |████████████████████████████████| 665kB 44.9MB/s \n",
            "\u001b[?25hCollecting rusenttokenize==0.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/25/4c/a2f00be5def774a3df2e5387145f1cb54e324607ec4a7e23f573645946e7/rusenttokenize-0.0.5-py3-none-any.whl\n",
            "Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (2.10.0)\n",
            "Collecting nltk==3.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 44.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (1.4.1)\n",
            "Collecting Cython==0.29.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/d1/4d3f8a7a920e805488a966cc6ab55c978a712240f584445d703c08b9f405/Cython-0.29.14-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 45.1MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.21.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/04/49633f490f726da6e454fddc8e938bbb5bfed2001681118d3814c219b723/scikit_learn-0.21.2-cp36-cp36m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 28.7MB/s \n",
            "\u001b[?25hCollecting fastapi==0.47.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/a7/4804d7abf8a1544d079d50650af872387154ebdac5bd07d54b2e60e2b334/fastapi-0.47.1-py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.2MB/s \n",
            "\u001b[?25hCollecting pytz==2019.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/73/fe30c2daaaa0713420d0382b16fbb761409f532c56bdcc514bf7b6262bb6/pytz-2019.1-py2.py3-none-any.whl (510kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 34.9MB/s \n",
            "\u001b[?25hCollecting pymorphy2==0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.1MB/s \n",
            "\u001b[?25hCollecting uvicorn==0.11.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/5f/2bc87272f189662e129ddcd4807ad3ef83128b4df3a3482335f5f9790f24/uvicorn-0.11.7-py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.2MB/s \n",
            "\u001b[?25hCollecting overrides==2.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/98/2430afd204c48ac0a529d439d7e22df8fa603c668d03456b5947cb59ec36/overrides-2.7.0.tar.gz\n",
            "Collecting sacremoses==0.0.35\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 38.4MB/s \n",
            "\u001b[?25hCollecting prometheus-client==0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b3/23/41a5a24b502d35a4ad50a5bb7202a5e1d9a0364d0c12f56db3dbf7aca76d/prometheus_client-0.7.1.tar.gz\n",
            "Collecting pytelegrambotapi==3.6.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/ab/99c606f69fcda57e35788b913dd34c9d9acb48dd26349141b3855dcf6351/pyTelegramBotAPI-3.6.7.tar.gz (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.5MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/a2/6565c5271a79e3c96d7a079053b4d8408a740d4bf365f0f5f244a807bd09/cryptography-3.2.1-cp35-abi3-manylinux2010_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 37.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from pyopenssl==19.1.0->deeppavlov) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas==0.25.3->deeppavlov) (2.8.1)\n",
            "Requirement already satisfied: dataclasses>=0.6; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from pydantic==1.3->deeppavlov) (0.7)\n",
            "Collecting yarl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/08/52b26b44bce7b818b410aee37c5e424c9ea420c557bca97dc2adac29b151/yarl-1.6.3-cp36-cp36m-manylinux2014_x86_64.whl (293kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 39.6MB/s \n",
            "\u001b[?25hCollecting aiormq<4,>=3.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0b/c4/dc5b9d50c15af2ee187974a5a0c3f20c06cce6559eea4c065d372e846b6a/aiormq-3.3.1-py3-none-any.whl\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->deeppavlov) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->deeppavlov) (2020.6.20)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->deeppavlov) (3.0.4)\n",
            "Collecting idna<2.9,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.2->deeppavlov) (0.17.0)\n",
            "Collecting starlette<=0.12.9,>=0.12.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/95/2220fe5bf287e693a6430d8ee36c681b0157035b7249ec08f8fb36319d16/starlette-0.12.9.tar.gz (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: dawg-python>=0.7 in /usr/local/lib/python3.6/dist-packages (from pymorphy2==0.8->deeppavlov) (0.7.2)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n",
            "Collecting pymorphy2-dicts<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 24.0MB/s \n",
            "\u001b[?25hCollecting httptools==0.1.*; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/a6/dc1e7e8f4049ab70d52c9690ec10652e268ab2542853033cc1d539594102/httptools-0.1.1-cp36-cp36m-manylinux1_x86_64.whl (216kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 47.8MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.0MB/s \n",
            "\u001b[?25hCollecting uvloop>=0.14.0; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/48/586225bbb02d3bdca475b17e4be5ce5b3f09da2d6979f359916c1592a687/uvloop-0.14.0-cp36-cp36m-manylinux2010_x86_64.whl (3.9MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 39.9MB/s \n",
            "\u001b[?25hCollecting websockets==8.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/d9/856af84843912e2853b1b6e898ac8b802989fcf9ecf8e8445a1da263bf3b/websockets-8.1-cp36-cp36m-manylinux2010_x86_64.whl (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (1.14.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from yarl->aio-pika==6.4.1->deeppavlov) (3.7.4.3)\n",
            "Collecting multidict>=4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/95/f89c97db4d27b24fce7c6fb6ef05228f347e43bbf67e4737a0660c8753fd/multidict-5.0.2-cp36-cp36m-manylinux2014_x86_64.whl (141kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 45.3MB/s \n",
            "\u001b[?25hCollecting pamqp==2.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/56/afa06143361e640c9159d828dadc95fc9195c52c95b4a97d136617b0166d/pamqp-2.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (2.20)\n",
            "Building wheels for collected packages: nltk, overrides, sacremoses, prometheus-client, pytelegrambotapi, starlette\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449906 sha256=f5b6aab47e0a77687d432a2745df7093395fab10b701c77f080bca95753eac6f\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-2.7.0-cp36-none-any.whl size=5601 sha256=b166a8a2eeb4158fa116d48655b615a2912fb39e84f9d811b212a6ff7aae21f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/7c/ef/80508418b67d87371c5b3de49e03eb22ee7c1d19affb5099f8\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=3c8077b2cb22c5387aaca737cf34567847d07aab31213b6cbff5893b21d970bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "  Building wheel for prometheus-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prometheus-client: filename=prometheus_client-0.7.1-cp36-none-any.whl size=41403 sha256=36abb48fc25c6a73b176b9fb77fe30cd11e41d74af953781d69662a987dfa8da\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/54/34/fd47cd9b308826cc4292b54449c1899a30251ef3b506bc91ea\n",
            "  Building wheel for pytelegrambotapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytelegrambotapi: filename=pyTelegramBotAPI-3.6.7-cp36-none-any.whl size=47179 sha256=6385dcf72ef196f699e3a8f5fb15c52725c90e7ff4b101a4d4c8fe2f7d3df013\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/40/18/8a34153f95ef0dc19e3954898e5a5079244b76a8afdd7d0ec5\n",
            "  Building wheel for starlette (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for starlette: filename=starlette-0.12.9-cp36-none-any.whl size=57245 sha256=714058a65cd2e93e21cfa4de48de1284520d979bb9b592998d74b5cbb8e6b3dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/51/5b/3828d52e185cafad941c4291b6f70894d0794be28c70addae5\n",
            "Successfully built nltk overrides sacremoses prometheus-client pytelegrambotapi starlette\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement pandas>=1.0.4, but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: cryptography, pyopenssl, pytz, numpy, pandas, pydantic, idna, multidict, yarl, pamqp, aiormq, aio-pika, requests, ruamel.yaml, rusenttokenize, nltk, Cython, scikit-learn, starlette, fastapi, pymorphy2-dicts, pymorphy2, httptools, h11, uvloop, websockets, uvicorn, overrides, sacremoses, prometheus-client, pytelegrambotapi, deeppavlov\n",
            "  Found existing installation: pytz 2018.9\n",
            "    Uninstalling pytz-2018.9:\n",
            "      Successfully uninstalled pytz-2018.9\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: pandas 1.1.4\n",
            "    Uninstalling pandas-1.1.4:\n",
            "      Successfully uninstalled pandas-1.1.4\n",
            "  Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Found existing installation: Cython 0.29.21\n",
            "    Uninstalling Cython-0.29.21:\n",
            "      Successfully uninstalled Cython-0.29.21\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Found existing installation: pymorphy2 0.9.1\n",
            "    Uninstalling pymorphy2-0.9.1:\n",
            "      Successfully uninstalled pymorphy2-0.9.1\n",
            "  Found existing installation: prometheus-client 0.8.0\n",
            "    Uninstalling prometheus-client-0.8.0:\n",
            "      Successfully uninstalled prometheus-client-0.8.0\n",
            "Successfully installed Cython-0.29.14 aio-pika-6.4.1 aiormq-3.3.1 cryptography-3.2.1 deeppavlov-0.13.0 fastapi-0.47.1 h11-0.9.0 httptools-0.1.1 idna-2.8 multidict-5.0.2 nltk-3.4.5 numpy-1.18.0 overrides-2.7.0 pamqp-2.3.0 pandas-0.25.3 prometheus-client-0.7.1 pydantic-1.3 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 pyopenssl-19.1.0 pytelegrambotapi-3.6.7 pytz-2019.1 requests-2.22.0 ruamel.yaml-0.15.100 rusenttokenize-0.0.5 sacremoses-0.0.35 scikit-learn-0.21.2 starlette-0.12.9 uvicorn-0.11.7 uvloop-0.14.0 websockets-8.1 yarl-1.6.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "idna",
                  "numpy",
                  "pandas",
                  "pymorphy2",
                  "pytz",
                  "requests"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2020-11-20 03:45:11.775 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'ner_ontonotes' as '/usr/local/lib/python3.6/dist-packages/deeppavlov/configs/ner/ner_ontonotes.json'\n",
            "Collecting tensorflow==1.15.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/d9/fd234c7bf68638423fb8e7f44af7fcfce3bcaf416b51e6d902391e47ec43/tensorflow-1.15.2-cp36-cp36m-manylinux2010_x86_64.whl (110.5MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5MB 87kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (3.12.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.33.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.18.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.8.1)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 34.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.10.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.2.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 38.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (3.3.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.35.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.15.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.2) (50.3.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.3.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.4.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7542 sha256=640135609a8951ee6b2925bb7a1814b02ec52b1732b4b8fe8cfbd04bef76ef30\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, gast, tensorflow-estimator, keras-applications, tensorflow\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.2 tensorflow-estimator-1.15.1\n",
            "Collecting gensim==3.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/dd/112bd4258cee11e0baaaba064060eb156475a42362e59e3ff28e7ca2d29d/gensim-3.8.1-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 38.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1) (1.18.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim==3.8.1) (2.22.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim==3.8.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim==3.8.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim==3.8.1) (2020.6.20)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim==3.8.1) (2.8)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.8.1\n",
            "2020-11-20 03:46:07.495 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'ner_ontonotes_bert' as '/usr/local/lib/python3.6/dist-packages/deeppavlov/configs/ner/ner_ontonotes_bert.json'\n",
            "Collecting git+https://github.com/deepmipt/bert.git@feat/multi_gpu\n",
            "  Cloning https://github.com/deepmipt/bert.git (to revision feat/multi_gpu) to /tmp/pip-req-build-iztun4ok\n",
            "  Running command git clone -q https://github.com/deepmipt/bert.git /tmp/pip-req-build-iztun4ok\n",
            "Building wheels for collected packages: bert-dp\n",
            "  Building wheel for bert-dp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-dp: filename=bert_dp-1.0-cp36-none-any.whl size=23581 sha256=68bb5a5f08f52a350cc2662701410bd3e91218133fd4e971b94df805fe6b1311\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zuzczjex/wheels/1e/41/94/886107eaf932532594886fd8bfc9cb9d4db632e94add49d326\n",
            "Successfully built bert-dp\n",
            "Installing collected packages: bert-dp\n",
            "Successfully installed bert-dp-1.0\n",
            "Requirement already satisfied: tensorflow==1.15.2 in /usr/local/lib/python3.6/dist-packages (1.15.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.0.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.12.1)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (3.12.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.33.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.35.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.18.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.15.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.3.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (50.3.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os8ll8GNAO0V",
        "outputId": "c1669ef8-bb49-41b8-b51e-9210ba6fa277"
      },
      "source": [
        "from deeppavlov import configs, build_model\n",
        "\n",
        "ner_model = build_model(configs.ner.ner_rus_bert, download=True)\n",
        "result = ner_model([TEXT])\n",
        "for i in range(len(result[0][0])):\n",
        "     if result [1][0][i] != 'O':\n",
        "        print(result[0][0][i], result[1][0][i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-20 03:46:16.406 INFO in 'deeppavlov.core.data.utils'['utils'] at line 94: Downloading from http://files.deeppavlov.ai/deeppavlov_data/ner_rus_bert_v1.tar.gz to /root/.deeppavlov/ner_rus_bert_v1.tar.gz\n",
            "100%|██████████| 1.32G/1.32G [03:36<00:00, 6.09MB/s]\n",
            "2020-11-20 03:49:53.165 INFO in 'deeppavlov.core.data.utils'['utils'] at line 268: Extracting /root/.deeppavlov/ner_rus_bert_v1.tar.gz archive into /root/.deeppavlov/models\n",
            "2020-11-20 03:50:08.128 INFO in 'deeppavlov.core.data.utils'['utils'] at line 94: Downloading from http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_v1.tar.gz to /root/.deeppavlov/downloads/rubert_cased_L-12_H-768_A-12_v1.tar.gz\n",
            " 50%|█████     | 335M/666M [00:53<00:56, 5.82MB/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44u6tZh6ItYw"
      },
      "source": [
        "def deeppavlov_result(TEXT):\n",
        "    result = ner_model([TEXT])\n",
        "    for i in range(len(result[0][0])):\n",
        "        if result [1][0][i] != 'O':\n",
        "            print(result[0][0][i], result[1][0][i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOSYZ_SHpNEF"
      },
      "source": [
        "## TEXT1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76PWX6mAojXH"
      },
      "source": [
        "natasha_result(TEXT1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtP09L_ipIU3"
      },
      "source": [
        "pullenty_result(TEXT1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cmx0bn1noj6H"
      },
      "source": [
        "stanza_result(TEXT1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELaTRvZGpAOh"
      },
      "source": [
        "polyglot_result(TEXT1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xADJeDOokQn"
      },
      "source": [
        "deeppavlov_result(TEXT1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDCf4jp_pYIp"
      },
      "source": [
        "## TEXT2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDdOghW4okHn"
      },
      "source": [
        "natasha_result(TEXT2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n27BrZpNoj0C"
      },
      "source": [
        "pullenty_result(TEXT2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyPFMxQ-ojsr"
      },
      "source": [
        "stanza_result(TEXT2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANO81bUqpgJ4"
      },
      "source": [
        "polyglot_result(TEXT2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKUqHYhUpipf"
      },
      "source": [
        "deeppavlov_result(TEXT2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJxCpTdKp-jF"
      },
      "source": [
        "## TEXT3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajCoZt4hp2lB"
      },
      "source": [
        "natasha_result(TEXT3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se4PaG-Pp3HS"
      },
      "source": [
        "pullenty_result(TEXT3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9-wsFCkp274"
      },
      "source": [
        "stanza_result(TEXT3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruz684-Sp2wH"
      },
      "source": [
        "polyglot_result(TEXT3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUhqrcEspzwO"
      },
      "source": [
        "deeppavlov_result(TEXT3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97OApko4AO0d"
      },
      "source": [
        "## Метрики\n",
        "\n",
        "Большой соблазн использовать здесь Accuracy, конечно, однако для разных подходов и метрики должны опираться на разные вещи. Что, если pullenty связывает нам сущности, а одна из библиотек, тем не менее, связывать их не пытается, но выделяет лучше?\n",
        "Accuracy -- возможное и, наверное, относительно неплохое решение при двух условиях:\n",
        "1) Сравнивать необходимо не просто анализ в целом, а по категориям. Места отдельно, людей отдельно, организации отдельно. Иначе не так ясно видно, где именно проседает выбранный теггер.\n",
        "2) Возможно, будет лучше разбивать ошибки по категориям, чтобы понимать, где не отмечена сущность, где отмечена лишняя, где неправильно определены границы, а где не тот тип. \n",
        "3) Теоретический скорее вопрос. Идеалом, возможно, была бы многослойная разметка, где сущности накладываются друг на друга. Например, 'доцент МГУ' было бы разобрано как должность на одном уровне и как содержащее в себе организацию -- на другом. Но тогда структура полученных тегов будет достаточно сложной, хотя именно это и пытается сделать pullenty. Так вот, один из теггеров может склоняться скорее к одному уровню, а второй -- к другому, и даже одна и та же модель после обучения может иметь некоторые изменения именно в этом. Поэтому здесь нужно или размечать тренировочный датасет вариативно, или понять, какой именно уклон нам нужен. Тем не менее, мне кажется, этот аспект всё равно следует не выпускать из внимания.\n",
        "4) Ошибки необходимо учитывать и первого, и второго рода.\n",
        "\n",
        "В таком случае любая метрика, соответствующая этим параметрам, подойдёт. Естественно, качество оценивать именно на дневниках и выбирать ту метрику, которая при этом достаточно нересурсозатратна.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUaycTLEAO0e"
      },
      "source": [
        "## Итоги"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUDzAebcAO0f"
      },
      "source": [
        "Для мастерской я вижу нескольку путей и неясных мест, и здесь попробую предложить варианты.\n",
        "1. Глубина разметки.  Хотим ли мы несколько уровней? Нужно ли, чтобы внутри отмеченного \"Александр Сергеевич Пушкин\" как единого человека содержалось то, что это имя, отчество и фамилия? Нужно ли выделить в конструкции из притяжательного имени и места и человека, и место? Нужна ли какая-то связь между тем и другим?\n",
        "2. Количество и обширность тегов. Хотим ли мы, чтобы выделялись только имена собственные, названия мест и время? Или мы хотим поставить в соответствие то, что Киевская Русь -- страна, а Куйбышев -- город? Нужны ли нам здесь должности и всяческие наименования людей?\n",
        "3. Некоторые из представленных решений имеют возможность дообучения. Таким образом, из собственных решений можно дообучать это или пытаться создать что-то из меньших частей кода? Или вообще применить то же приём, что при создании Наташи, когда более лёгкая модель обучалась на реультатах работы более тяжёлой.\n",
        "4. Насколько важны вес модели и скорость её работы?\n"
      ]
    }
  ]
}